# Complete Ablation Study: 12 Runs (2 architectures Ã— methods Ã— wavelets Ã— 4 modalities)
# Research Question: Which components of Fast-cWDM contribute most to performance?
# 
# Architecture + Method combinations:
#   UNet (all 3 methods): 9 configs
#   SWIN (direct only): 3 configs
#   Total: 12 configurations
#
# Configuration Matrix (12 runs):
#   UNet Direct Regression (3 runs):
#     1. unet + direct + nowavelet    (baseline: UNet direct regression, image space)
#     2. unet + direct + haar         (UNet direct regression with Haar wavelets)
#     3. unet + direct + db2          (UNet direct regression with Daubechies-2)
#   
#   SWIN Direct Regression (3 runs):
#     4. swin + direct + nowavelet    (SWIN direct regression, image space)
#     5. swin + direct + haar         (SWIN direct regression with Haar wavelets)
#     6. swin + direct + db2          (SWIN direct regression with Daubechies-2)
#   
#   UNet Fast Diffusion (3 runs):
#     7. unet + diffusion_fast + nowavelet  (fast diffusion UNet, image space)
#     8. unet + diffusion_fast + haar       (YOUR BraTS 2025 SUBMISSION - 0.872 WT Dice)
#     9. unet + diffusion_fast + db2        (fast diffusion UNet with db2)
#   
#   UNet Standard Diffusion (3 runs):
#     10. unet + diffusion_standard + nowavelet (standard DDPM UNet, image space)
#     11. unet + diffusion_standard + haar   (standard DDPM UNet with Haar)
#     12. unet + diffusion_standard + db2    (standard DDPM UNet with db2)
#
# Note: Each run trains all 4 modalities sequentially (t1n, t1c, t2w, t2f)

project: fast-cwdm-ablation-study
program: app/scripts/train.py
command:
  - ${env}
  - PYTHONPATH=${PYTHONPATH}:./app
  - python
  - ${program}
  - ${args}
method: grid
metric:
  name: eval/dice_wt
  goal: maximize

parameters:
  # Fixed optimal hyperparameters from BraTS 2025 submission
  data_dir:
    value: ./datasets/BRATS2023/training
  dataset:
    value: brats
  image_size:
    value: 224
  batch_size:
    value: 1  # Reduced from 2 to avoid OOM on direct+null baseline
  lr:
    value: 1e-4
  num_channels:
    value: 48  # ðŸ”§ REDUCED from 64 to save memory (48â†’96â†’96â†’192â†’192 vs 64â†’128â†’128â†’256â†’256)
  in_channels:
    value: 32  # 8 wavelet components Ã— 4 modalities
  out_channels:
    value: 8   # 8 wavelet components for target
  channel_mult:
    value: "1,2,2,4,4"
  num_workers:
    value: 12
  dims:
    value: 3
  attention_resolutions:
    value: ""
  num_heads:
    value: 1
  bottleneck_attention:
    value: false
  num_groups:
    value: 16  # ðŸ”§ REDUCED from 32 to match num_channels=48 (must be divisible)
  
  # Training configuration
  diffusion_steps:
    value: 100
  save_interval:
    value: 10000
  log_interval:
    value: 100
  special_checkpoint_steps:
    value: "47500,100000,150000,200000"
  save_to_wandb:
    value: true
  
  # Memory optimizations (CRITICAL for 80GB A100)
  use_fp16:
    value: true  # Mixed precision training - saves ~50% memory
  use_checkpoint:
    value: true  # Gradient checkpointing - saves memory at cost of ~20% speed
  
  # ABLATION GRID DIMENSIONS (12 configurations)
  
  # Architecture: UNet (all methods) vs SWIN (direct only)
  architecture:
    values: ['unet', 'swin']
  
  # Method: Direct regression vs Diffusion approaches
  model_mode:
    values: ['direct', 'diffusion_fast', 'diffusion_standard']
  
  # Wavelet: nowavelet (baseline) vs Haar vs Daubechies-2
  wavelet:
    values: ['nowavelet', 'haar', 'db2']
  
  # Sample schedule (controlled by model_mode)
  sample_schedule:
    # Auto-set based on model_mode:
    # direct â†’ 'direct' (unused)
    # diffusion_fast â†’ 'sampled'
    # diffusion_standard â†’ 'direct'
    value: sampled  # Overridden by model_mode
  
  # Train all 4 modalities in sequence (or specify one)
  contr:
    values: ['t1n', 't1c', 't2w', 't2f']
  
  # Auto-computed based on wavelet
  use_freq:
    # null â†’ false (baseline UNet in image space)
    # haar/db2 â†’ true (WavUNet in wavelet space)
    value: true  # Overridden based on wavelet

# Expected results structure:
# 
# UNet Direct Regression (fast inference, single-pass):
#   Run 1: unet + direct + nowavelet (baseline UNet regression, image space)
#   Run 2: unet + direct + haar (UNet regression with Haar wavelets)
#   Run 3: unet + direct + db2 (UNet regression with Daubechies-2)
# 
# SWIN Direct Regression (fast inference, single-pass):
#   Run 4: swin + direct + nowavelet (SWIN regression, image space)
#   Run 5: swin + direct + haar (SWIN regression with Haar wavelets)
#   Run 6: swin + direct + db2 (SWIN regression with Daubechies-2)
# 
# UNet Fast Diffusion (importance sampled, T=100):
#   Run 7: unet + diffusion_fast + nowavelet (Fast-DDPM UNet baseline, image space)
#   Run 8: unet + diffusion_fast + haar (YOUR SUBMISSION - 0.872 WT Dice)
#   Run 9: unet + diffusion_fast + db2 (Fast-DDPM UNet with db2)
# 
# UNet Standard Diffusion (uniform schedule, T=100):
#   Run 10: unet + diffusion_standard + nowavelet (Standard DDPM UNet, image space)
#   Run 11: unet + diffusion_standard + haar (Standard DDPM UNet with Haar)
#   Run 12: unet + diffusion_standard + db2 (Standard DDPM UNet with db2)
#
# Each run trains all 4 modalities (t1n, t1c, t2w, t2f) for 200k iterations.
# Total: 12 model configurations Ã— 4 modalities = 48 checkpoints.
#
# NOTE: SWIN only supports direct regression (no diffusion). The sweep will
# automatically skip invalid combinations (swin + diffusion_*) via train.py logic.
