# Ablation Study: 12 Configurations (Architecture Ã— Method Ã— Wavelet)
# UNet: direct/diffusion_fast/diffusion_standard Ã— nowavelet/haar/db2 (9 configs)
# SWIN: direct Ã— nowavelet/haar/db2 (3 configs)
# Each config trains 4 modalities (t1n, t1c, t2w, t2f) for 200k iterations

project: fast-cwdm-ablation-study
program: app/scripts/train.py
command:
  - ${env}
  - PYTHONPATH=${PYTHONPATH}:./app
  - python
  - ${program}
  - ${args}
method: grid
metric:
  name: eval/dice_wt
  goal: maximize

parameters:
  # Fixed optimal hyperparameters from BraTS 2025 submission
  data_dir:
    value: ./datasets/BRATS2023/training
  dataset:
    value: brats
  image_size:
    value: 224
  batch_size:
    value: 1  # Reduced from 2 to avoid OOM on direct+null baseline
  lr:
    value: 1e-4
  num_channels:
    value: 48  # ðŸ”§ REDUCED from 64 to save memory (48â†’96â†’96â†’192â†’192 vs 64â†’128â†’128â†’256â†’256)
  in_channels:
    value: 32  # 8 wavelet components Ã— 4 modalities
  out_channels:
    value: 8   # 8 wavelet components for target
  channel_mult:
    value: "1,2,2,4,4"
  num_workers:
    value: 12
  dims:
    value: 3
  attention_resolutions:
    value: ""
  num_heads:
    value: 1
  bottleneck_attention:
    value: false
  num_groups:
    value: 16  # ðŸ”§ REDUCED from 32 to match num_channels=48 (must be divisible)
  
  # Training configuration
  max_iterations:
    value: 1000
  diffusion_steps:
    value: 100
  save_interval:
    value: 10000
  log_interval:
    value: 100
  special_checkpoint_steps:
    value: "47500,100000,150000,200000"
  save_to_wandb:
    value: true
  
  # Memory optimizations
  use_fp16:
    value: true
  use_checkpoint:
    value: true
  
  # Ablation grid
  architecture:
    values: ['unet', 'swin']
  
  model_mode:
    values: ['direct', 'diffusion_fast', 'diffusion_standard']
  
  wavelet:
    values: ['nowavelet', 'haar', 'db2']
  
  sample_schedule:
    value: sampled
  
  contr:
    values: ['t1n', 't1c', 't2w', 't2f']
  
  use_freq:
    value: true


