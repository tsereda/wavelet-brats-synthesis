#!/bin/bash
set -e Â # Exit on error

echo "========================================="
echo "Wavelet BRATS Synthesis - W&B Sweep Agent"
echo "========================================="

# Parse command line arguments
RESUME_RUN=""
CHECKPOINT_DIR="./checkpoints"

while [[ $# -gt 0 ]]; do
Â  case $1 in
Â  Â  --resume_run)
Â  Â  Â  RESUME_RUN="$2"
Â  Â  Â  shift 2
Â  Â  Â  ;;
Â  Â  --checkpoint_dir)
Â  Â  Â  CHECKPOINT_DIR="$2"
Â  Â  Â  shift 2
Â  Â  Â  ;;
Â  Â  *)
Â  Â  Â  echo "Unknown option: $1"
Â  Â  Â  shift
Â  Â  Â  ;;
Â  esac
done

# Determine working directory
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
cd "$SCRIPT_DIR"

echo "Working directory: $(pwd)"
echo "Checkpoint directory: $CHECKPOINT_DIR"

# Install system dependencies
echo "[1/7] Installing system dependencies..."
apt-get update && apt-get install -y \
Â  Â  p7zip-full \
Â  Â  wget \
Â  Â  git \
Â  Â  unzip \
Â  Â  || echo "System packages already installed"

# Set environment variables
export REPO_PATH="$(pwd)"
export PYTHONPATH="$(pwd)"
export PYTHONUNBUFFERED=1
export PYTHONIOENCODING=UTF-8
# New environment variables for auto-resume
export LATEST_CHECKPOINT_FILE=""
export LATEST_CHECKPOINT_STEP=""

echo "REPO_PATH: $REPO_PATH"
echo "PYTHONPATH: $PYTHONPATH"

# Install Python dependencies
echo "[2/7] Installing Python dependencies..."
pip install --no-cache-dir \
Â  Â  pyyaml \
Â  Â  torch \
Â  Â  torchvision \
Â  Â  tqdm \
Â  Â  numpy \
Â  Â  nibabel \
Â  Â  wandb \
Â  Â  matplotlib \
Â  Â  blobfile \
Â  Â  tensorboard \
Â  Â  monai \
Â  Â  pillow

# Verify critical files exist
echo "[3/7] Verifying repository structure..."
if [ ! -f "app/scripts/train.py" ]; then
Â  Â  echo "ERROR: app/scripts/train.py not found!"
Â  Â  echo "Current directory: $(pwd)"
Â  Â  echo "Contents:"
Â  Â  ls -la
Â  Â  echo "Scripts directory:"
Â  Â  ls -la app/scripts/ 2>/dev/null || echo "app/scripts/ directory not found!"
Â  Â  exit 1
fi
echo "âœ“ app/scripts/train.py found"

# Setup directories
echo "[4/7] Setting up directories..."
mkdir -p ./datasets/BRATS2023/training
mkdir -p ./datasets/BRATS2023/validation
mkdir -p "$CHECKPOINT_DIR"
mkdir -p ./logs
mkdir -p ./wandb

# Prepare data
echo "[5/7] Preparing data..."
if [ ! -d "datasets/BRATS2023/training" ] || [ -z "$(ls -A datasets/BRATS2023/training)" ]; then
Â  Â  echo "Extracting BRATS training data..."
Â  Â  
Â  Â  if [ -f "/data/ASNR-MICCAI-BraTS2023-GLI-MET-TrainingData.tar.gz" ]; then
Â  Â  Â  Â  echo "Found training data archive, extracting..."
Â  Â  Â  Â  7z x /data/ASNR-MICCAI-BraTS2023-GLI-MET-TrainingData.tar.gz -o.
Â  Â  Â  Â  7z x ASNR-MICCAI-BraTS2023-GLI-MET-TrainingData.tar -o.
Â  Â  Â  Â  mv ASNR-MICCAI-BraTS2023-GLI-MET-TrainingData/* datasets/BRATS2023/training/
Â  Â  Â  Â  rm -f ASNR-MICCAI-BraTS2023-GLI-MET-TrainingData.tar*
Â  Â  else
Â  Â  Â  Â  echo "WARNING: Training data not found at /data/ASNR-MICCAI-BraTS2023-GLI-MET-TrainingData.tar.gz"
Â  Â  fi
Â  Â  
Â  Â  if [ -f "/data/ASNR-MICCAI-BraTS2023-GLI-MET-ValidationData.tar.gz" ]; then
Â  Â  Â  Â  echo "Found validation data archive, extracting..."
Â  Â  Â  Â  7z x /data/ASNR-MICCAI-BraTS2023-GLI-MET-ValidationData.tar.gz -o.
Â  Â  Â  Â  7z x ASNR-MICCAI-BraTS2023-GLI-MET-ValidationData.tar -o.
Â  Â  Â  Â  mv ASNR-MICCAI-BraTS2023-GLI-MET-ValidationData/* datasets/BRATS2023/validation/
Â  Â  Â  Â  rm -f ASNR-MICCAI-BraTS2023-GLI-MET-ValidationData.tar*
Â  Â  else
Â  Â  Â  Â  echo "WARNING: Validation data not found at /data/ASNR-MICCAI-BraTS2023-GLI-MET-ValidationData.tar.gz"
Â  Â  fi
Â  Â  
Â  Â  # Clean hidden files
Â  Â  find datasets/BRATS2023/training -name ".*" -delete 2>/dev/null || true
Â  Â  find datasets/BRATS2023/validation -name ".*" -delete 2>/dev/null || true
Â  Â  
Â  Â  # Remove problematic patient if exists
Â  Â  rm -rf datasets/BRATS2023/training/BraTS-MET-00232-000/ 2>/dev/null || true
else
Â  Â  echo "BRATS data already prepared"
fi

# Setup checkpoints
echo "[5.5/7] Setting up checkpoints..."

# Check if we need to resume from W&B
if [ -n "$RESUME_RUN" ]; then
Â  Â  echo "ğŸ”„ Resuming from W&B run: $RESUME_RUN"
Â  Â  
Â  Â  # Download checkpoints from W&B run
Â  Â  echo "Downloading checkpoints from W&B..."
Â  Â  python3 << EOF
import wandb
import os
import sys

run_path = "$RESUME_RUN"
checkpoint_dir = "$CHECKPOINT_DIR"

try:
Â  Â  # Initialize API
Â  Â  api = wandb.Api()
Â  Â  
Â  Â  # Get the run
Â  Â  print(f"Fetching run: {run_path}")
Â  Â  run = api.run(run_path)
Â  Â  
Â  Â  # Download all checkpoint files
Â  Â  checkpoint_files = [f for f in run.files() if f.name.endswith('.pt')]
Â  Â  
Â  Â  if not checkpoint_files:
Â  Â  Â  Â  print("âŒ No checkpoint files found in this run!")
Â  Â  Â  Â  sys.exit(1)
Â  Â  
Â  Â  print(f"Found {len(checkpoint_files)} checkpoint files")
Â  Â  
Â  Â  os.makedirs(checkpoint_dir, exist_ok=True)
Â  Â  
Â  Â  for file in checkpoint_files:
Â  Â  Â  Â  print(f" Â Downloading: {file.name}")
Â  Â  Â  Â  file.download(root=checkpoint_dir, replace=True)
Â  Â  
Â  Â  print(f"âœ… Downloaded {len(checkpoint_files)} checkpoints to {checkpoint_dir}")
Â  Â  
except Exception as e:
Â  Â  print(f"âŒ Error downloading checkpoints: {e}")
Â  Â  import traceback
Â  Â  traceback.print_exc()
Â  Â  sys.exit(1)
EOF
Â  Â  
Â  Â  if [ $? -ne 0 ]; then
Â  Â  Â  Â  echo "âŒ Failed to download checkpoints from W&B"
Â  Â  Â  Â  exit 1
Â  Â  fi
Â  Â  
else
Â  Â  # Check if checkpoint archive exists (fallback)
Â  Â  if [ -f "/data/400kCheckpoints.zip" ]; then
Â  Â  Â  Â  echo "Found checkpoint archive at /data/400kCheckpoints.zip"
Â  Â  Â  Â  if [ ! -d "$CHECKPOINT_DIR" ] || [ -z "$(ls -A $CHECKPOINT_DIR/*.pt 2>/dev/null)" ]; then
Â  Â  Â  Â  Â  Â  echo "Extracting checkpoints..."
Â  Â  Â  Â  Â  Â  unzip -o /data/400kCheckpoints.zip -d ./
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  # Copy checkpoints to the checkpoint directory
Â  Â  Â  Â  Â  Â  for modality in t1n t1c t2w t2f; do
Â  Â  Â  Â  Â  Â  Â  Â  if [ -d "${modality}" ]; then
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  echo "Copying ${modality} checkpoints..."
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  cp ${modality}/brats_*.pt "$CHECKPOINT_DIR/" 2>/dev/null || true
Â  Â  Â  Â  Â  Â  Â  Â  fi
Â  Â  Â  Â  Â  Â  done
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  echo "âœ“ Checkpoints extracted to $CHECKPOINT_DIR"
Â  Â  Â  Â  else
Â  Â  Â  Â  Â  Â  echo "âœ“ Checkpoints already exist in $CHECKPOINT_DIR"
Â  Â  Â  Â  fi

Â  Â  else # <--- START OF NEW LOGIC: Find latest local checkpoint
Â  Â  Â  Â  echo "Searching for the latest local checkpoint in $CHECKPOINT_DIR..."
Â  Â  Â  Â  
Â  Â  Â  Â  # Find all .pt files, strip their path, look for the step number (first large sequence of digits), 
Â  Â  Â  Â  # and sort numerically to find the largest one.
Â  Â  Â  Â  LATEST_STEP=$(find "$CHECKPOINT_DIR" -maxdepth 1 -type f -name "*.pt" | \
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  while read f; do 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  basename "$f" | grep -oP '\d+' | head -1; 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  done 2>/dev/null | sort -rn | head -1)

Â  Â  Â  Â  FULL_FILENAME=""
Â  Â  Â  Â  if [ -n "$LATEST_STEP" ]; then
Â  Â  Â  Â  Â  Â  # Find the full filename corresponding to the largest step number
Â  Â  Â  Â  Â  Â  FULL_FILENAME=$(find "$CHECKPOINT_DIR" -maxdepth 1 -type f -name "*${LATEST_STEP}*.pt" | head -1)
Â  Â  Â  Â  fi

Â  Â  Â  Â  if [ -n "$FULL_FILENAME" ]; then
Â  Â  Â  Â  Â  Â  echo "âœ… Found latest checkpoint: $(basename "$FULL_FILENAME") (Step: $LATEST_STEP)"
Â  Â  Â  Â  Â  Â  # Export the checkpoint file and step as environment variables for the final training script
Â  Â  Â  Â  Â  Â  export LATEST_CHECKPOINT_FILE="$FULL_FILENAME"
Â  Â  Â  Â  Â  Â  export LATEST_CHECKPOINT_STEP="$LATEST_STEP"
Â  Â  Â  Â  Â  Â  echo "Exported LATEST_CHECKPOINT_FILE and LATEST_CHECKPOINT_STEP."
Â  Â  Â  Â  else
Â  Â  Â  Â  Â  Â  echo "âš ï¸ No checkpoint files found in $CHECKPOINT_DIR. Starting from scratch."
Â  Â  Â  Â  fi
Â  Â  fi # <--- END OF NEW LOGIC
fi

echo "Training patients: $(ls datasets/BRATS2023/training/ 2>/dev/null | wc -l)"
echo "Validation patients: $(ls datasets/BRATS2023/validation/ 2>/dev/null | wc -l)"

# Verify environment
echo "[6/7] Verifying environment..."
python -c "import torch; print(f'PyTorch version: {torch.__version__}')"
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
if python -c "import torch; torch.cuda.is_available()" 2>/dev/null; then
Â  Â  python -c "import torch; print(f'GPU count: {torch.cuda.device_count()}')"
Â  Â  python -c "import torch; print(f'GPU name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')"
fi
python -c "import wandb; print(f'W&B version: {wandb.__version__}')"

# Verify W&B authentication
echo "[7/7] Verifying W&B authentication..."
if [ -z "$WANDB_API_KEY" ]; then
Â  Â  echo "WARNING: WANDB_API_KEY not set, attempting interactive login..."
Â  Â  python -c "import wandb; wandb.login()"
else
Â  Â  echo "âœ“ WANDB_API_KEY is set"
Â  Â  python -c "import wandb; wandb.login()"
fi

echo "========================================="
echo "Setup complete!"
echo "========================================="

# Check for required environment variables
if [ -z "$SWEEP_ID" ]; then
Â  Â  echo ""
Â  Â  echo "WARNING: SWEEP_ID environment variable not set"
Â  Â  echo "You have two options:"
Â  Â  echo ""
Â  Â  echo "1. Run with W&B sweep:"
Â  Â  echo " Â  export SWEEP_ID='your-entity/your-project/sweep-id'"
Â  Â  echo " Â  ./run.sh"
Â  Â  echo ""
Â  Â  echo "2. Run normal training:"
Â  Â  # Modified to show how to use the exported variables for normal run
Â  Â  if [ -n "$LATEST_CHECKPOINT_FILE" ]; then
Â  Â  Â  Â  echo " Â  python app/scripts/train.py --data_dir=./datasets/BRATS2023/training --contr=t1n --lr=1e-5 \\"
Â  Â  Â  Â  echo " Â  Â  --resume_checkpoint=\"\$LATEST_CHECKPOINT_FILE\" --resume_step=\"\$LATEST_CHECKPOINT_STEP\""
Â  Â  else
Â  Â  Â  Â  echo " Â  python app/scripts/train.py --data_dir=./datasets/BRATS2023/training --contr=t1n --lr=1e-5"
Â  Â  fi
Â  Â  echo ""
Â  Â  echo "3. Resume from W&B run:"
Â  Â  echo " Â  ./run.sh --resume_run entity/project/run_id"
Â  Â  echo ""
Â  Â  exit 1
fi

if [ -z "$WANDB_ENTITY" ]; then
Â  Â  echo "WARNING: WANDB_ENTITY not set, using default from sweep config"
fi

if [ -z "$WANDB_PROJECT" ]; then
Â  Â  echo "WARNING: WANDB_PROJECT not set, using default from sweep config"
fi

echo ""
echo "Starting W&B Sweep Agent..."
echo "Sweep ID: $SWEEP_ID"
echo "Entity: ${WANDB_ENTITY:-'(from sweep config)'}"
echo "Project: ${WANDB_PROJECT:-'(from sweep config)'}"

if [ -n "$RESUME_RUN" ]; then
Â  Â  echo "Resumed from W&B run: $RESUME_RUN"
fi

# Inform the user about local checkpoint status
if [ -n "$LATEST_CHECKPOINT_FILE" ]; then
Â  Â  echo "Local resume candidate: ${LATEST_CHECKPOINT_FILE} (Step: ${LATEST_CHECKPOINT_STEP})"
Â  Â  echo "The W&B agent's runs will automatically use these local checkpoints if configured to resume."
fi

echo "========================================="
echo ""

# The W&B agent will execute the training command defined in your sweep YAML.
# To utilize the LATEST_CHECKPOINT_FILE and LATEST_CHECKPOINT_STEP, your 
# sweep config's command must refer to these environment variables.
wandb agent "$SWEEP_ID"