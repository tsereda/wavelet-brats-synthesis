apiVersion: v1
kind: Pod
metadata:
  name: 5-fast-cwdm
spec:
  nodeSelector:
    #topology.kubernetes.io/region: us-west
    nautilus.io/linstor: "true"
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: nvidia.com/gpu.product
            operator: In
            values:
            - NVIDIA-A10
            - NVIDIA-GeForce-RTX-3090
            - NVIDIA-GeForce-RTX-4090
            - NVIDIA-TITAN-RTX
            - NVIDIA-RTX-A5000
            - Quadro-RTX-6000
            - Tesla-V100-SXM2-32GB
            - NVIDIA-A40
            - NVIDIA-L40
            - NVIDIA-RTX-A6000
            - Quadro-RTX-8000

  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
    - key: nvidia.com/gpu
      operator: Exists
      effect: PreferNoSchedule
  containers:
    - name: brats-processing
      image: gitlab-registry.nrp-nautilus.io/prp/jupyter-stack/prp
      env:
        - name: REPO_PATH
          value: /app/brats-synthesis
        - name: PYTHONPATH
          value: /app/brats-synthesis
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
        - name: nnUNet_raw
          value: /app/nnunet/raw
        - name: nnUNet_preprocessed
          value: /app/nnunet/preprocessed
        - name: nnUNet_results
          value: /app/nnunet/results
        - name: PYTHONUNBUFFERED
          value: "1"
        - name: PYTHONIOENCODING
          value: "UTF-8"
        - name: WANDB_API_KEY
          valueFrom:
            secretKeyRef:
              name: wandb-credentials
              key: api-key
        - name: WANDB_PROJECT
          value: "fast-cwmd"
        - name: WANDB_ENTITY
          value: "timgsereda"
        - name: WANDB_MODE
          value: "online"

      command: ["/bin/bash", "-c"]
      args:
        - |
          sudo rm -r /data/checkpoints/
          sudo mkdir /data/checkpoints/
          sudo chmod 777 /data/checkpoints/

          git clone -b ssim-loss https://github.com/tsereda/brats-synthesis.git ${REPO_PATH}
          cd ${REPO_PATH}
          
          sudo apt-get update && sudo apt-get install -y p7zip-full wget git
          
          # Create target directories
          mkdir -p datasets/BRATS2023/training
          mkdir -p datasets/BRATS2023/validation
          
          ls -la /data/

          7z x /data/ASNR-MICCAI-BraTS2023-GLI-MET-TrainingData.tar.gz
          7z x /data/ASNR-MICCAI-BraTS2023-GLI-MET-ValidationData.tar.gz

          7z x ASNR-MICCAI-BraTS2023-GLI-MET-TrainingData.tar
          7z x ASNR-MICCAI-BraTS2023-GLI-MET-ValidationData.tar

          mv ASNR-MICCAI-BraTS2023-GLI-MET-TrainingData/* datasets/BRATS2023/training/
          mv ASNR-MICCAI-BraTS2023-GLI-MET-ValidationData/* datasets/BRATS2023/validation/

          find datasets/BRATS2023/training -name ".*" -delete 2>/dev/null || true
          find datasets/BRATS2023/validation -name ".*" -delete 2>/dev/null || true

          echo "Training patients: $(ls datasets/BRATS2023/training/ | wc -l)"
          echo "Validation patients: $(ls datasets/BRATS2023/validation/ | wc -l)"

          pip install pyyaml torch tqdm numpy nibabel wandb matplotlib blobfile tensorboard monai
          
          python -c "import wandb; print(f'W&B version: {wandb.__version__}')"
          python -c "import torch; print(f'GPUs available: {torch.cuda.device_count()}')"

          conda init bash
          source ~/.bashrc

          mamba env create -f environment.yml

          conda activate cwdm

          conda info --envs
          python --version

          echo "Training patients: $(ls datasets/BRATS2023/training/ | wc -l)"
          echo "Validation patients: $(ls datasets/BRATS2023/validation/ | wc -l)"
          echo "Sample training patient files: $(ls datasets/BRATS2023/training/$(ls datasets/BRATS2023/training/ | head -1)/ | wc -l)"
          
          mv datasets/ fast_cwdm/
          cd fast_cwdm
          rm -rf datasets/BRATS2023/training/BraTS-MET-00232-000/
          cp /data/BraTSChekpoints.zip BraTSChekpoints.zip 
          unzip BraTSChekpoints.zip 
          ls BraTSCheckpoints/t1n
          #python scripts/check_data_integrity.py --data_dir datasets/BRATS2023/training
          cp -r BraTSCheckpoints/t1n ./t1n
          
          # Rename optimizer checkpoint to match expected resume step
          echo "Renaming optimizer checkpoint..."
          if ls ./t1n/opt*.pt 1> /dev/null 2>&1; then
            OLD_OPT=$(ls ./t1n/opt*.pt | head -1)
            mv "$OLD_OPT" "./t1n/opt074500.pt"
            echo "Renamed $OLD_OPT to ./t1n/opt074500.pt"
          else
            echo "No optimizer checkpoint found to rename"
          fi
          
          #MODALITIES=("t1n" "t1c" "t2w" "t2f")
          bash run.sh --sampling-strategy sampled --timesteps 100 --mode train --train_modality t1n --resume_checkpoint ./t1n/brats_*.pt --resume_step 74500
          ls -la /data/checkpoints/

          tail -f /dev/null
     
      volumeMounts:
        - name: workspace
          mountPath: /app
        - name: data
          mountPath: /data
        - name: shm
          mountPath: /dev/shm
    
      resources:
        requests:
          memory: 24Gi
          cpu: "12"
          nvidia.com/gpu: "1"
        limits:
          memory: 32Gi
          cpu: "16"
          nvidia.com/gpu: "1"
 
  volumes:
    - name: workspace
      emptyDir:
        sizeLimit: 50Gi
    - name: data
      persistentVolumeClaim:
        claimName: brats2025-5
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 16Gi
 
  restartPolicy: Never